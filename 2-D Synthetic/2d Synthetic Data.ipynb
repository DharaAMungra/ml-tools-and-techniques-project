{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "lQuiOiDjyDS5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "_mlN7QVCIZzf"
   },
   "outputs": [],
   "source": [
    "device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "UNYOGZLowbXc"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X,y = make_moons((9000,1000),noise=0.1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 587
    },
    "id": "q7T0UPyExDjd",
    "outputId": "a2c5a773-2e97-4020-f3f3-43b3af7df1dd"
   },
   "outputs": [],
   "source": [
    "# from sklearn.datasets import make_classification\n",
    "# X,y = make_classification(n_samples=10000, n_features=2, n_informative=2, \n",
    "#                     n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=2,\n",
    "#                           class_sep=2,\n",
    "#                    flip_y=0,weights=[0.5,0.5], random_state=17)\n",
    "\n",
    "\n",
    "\n",
    "# f, (ax1,ax2) = plt.subplots(nrows=1, ncols=2,figsize=(20,8))\n",
    "# sns.scatterplot(X[:,0],X[:,1],hue=y,ax=ax1);\n",
    "# ax1.set_title(\"No Imbalance\");\n",
    "\n",
    "# X,y = make_classification(n_samples=1000, n_features=2, n_informative=2, \n",
    "#                     n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=2,\n",
    "#                           class_sep=2,\n",
    "#                    flip_y=0,weights=[0.99,0.01], random_state=17)\n",
    "# sns.scatterplot(X[:,0],X[:,1],hue=y,ax=ax2);\n",
    "# ax2.set_title(\"Imbalance 9:1 :: Negative:Postive\");\n",
    "\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "k22xv3m30RWv"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42,stratify=y)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.Tensor(X_train),torch.Tensor(y_train).long())\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.Tensor(X_test),torch.Tensor(y_test).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([2970,  330]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_test,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "1Qr5CJb6xrtD"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=20, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "nco2MYhZyH1c"
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "4c5-NpYDyW-P"
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "epochs = n_iters / (len(X) / batch_size)\n",
    "input_dim = 2\n",
    "output_dim = 2\n",
    "lr_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "-GLMjiJdzt4I"
   },
   "outputs": [],
   "source": [
    "def my_loss(pred, true, weight_list, device):\n",
    "\n",
    "    values = torch.unique(true,return_counts = True)[0].cpu().numpy()\n",
    "    counts = torch.unique(true,return_counts = True)[1].cpu().numpy()\n",
    "    count_dict = {values[i]:counts[i] for i in range(len(values))}\n",
    "    if (0 not in count_dict.keys()):\n",
    "      count_dict[0] = 0\n",
    "    if (1 not in count_dict.keys()):\n",
    "      count_dict[1] = 0\n",
    "\n",
    "    n1,n2 = count_dict[0],count_dict[1]\n",
    "    w1,w2 = weight_list\n",
    "    loss1 = torch.nn.CrossEntropyLoss(weight=torch.tensor([w1,w2], dtype=torch.float32).to(device))\n",
    "    return (loss1(pred, true) * (torch.tensor(w1) + torch.tensor(w2)))/((torch.tensor(n1)*torch.tensor(w1)) + (torch.tensor(n2)*torch.tensor(w2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "import random\n",
    "random.seed(0)\n",
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "tbXaU1RxyZv7"
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "zp2jWB4QydHU"
   },
   "outputs": [],
   "source": [
    "criterion = my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "d4FIsz-3ye-m"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S5Qlus_j5fwo",
    "outputId": "1aae0df9-6d19-4e1d-ed70-074861bde753"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 00500 | Train Loss: 0.1174 | Train Accuracy: 90.6269 | Accuracy: 90.5600 | F1_score: 0.6317\n",
      "Iteration: 01000 | Train Loss: 0.1171 | Train Accuracy: 90.5522 | Accuracy: 90.5000 | F1_score: 0.6310\n",
      "Iteration: 01500 | Train Loss: 0.0999 | Train Accuracy: 90.5224 | Accuracy: 90.4700 | F1_score: 0.6302\n",
      "Iteration: 02000 | Train Loss: 0.1277 | Train Accuracy: 90.4776 | Accuracy: 90.4300 | F1_score: 0.6295\n",
      "Iteration: 02500 | Train Loss: 0.1490 | Train Accuracy: 90.4478 | Accuracy: 90.4100 | F1_score: 0.6295\n",
      "Iteration: 03000 | Train Loss: 0.0625 | Train Accuracy: 90.4328 | Accuracy: 90.4000 | F1_score: 0.6295\n",
      "Iteration: 03500 | Train Loss: 0.1067 | Train Accuracy: 90.4328 | Accuracy: 90.3900 | F1_score: 0.6279\n",
      "Iteration: 04000 | Train Loss: 0.1583 | Train Accuracy: 90.3582 | Accuracy: 90.3300 | F1_score: 0.6272\n",
      "Iteration: 04500 | Train Loss: 0.1445 | Train Accuracy: 90.3582 | Accuracy: 90.3200 | F1_score: 0.6265\n",
      "Iteration: 05000 | Train Loss: 0.0412 | Train Accuracy: 90.3433 | Accuracy: 90.2900 | F1_score: 0.6250\n",
      "Iteration: 05500 | Train Loss: 0.1577 | Train Accuracy: 90.3134 | Accuracy: 90.2700 | F1_score: 0.6250\n",
      "Iteration: 06000 | Train Loss: 0.1097 | Train Accuracy: 90.2687 | Accuracy: 90.2400 | F1_score: 0.6250\n",
      "Iteration: 06500 | Train Loss: 0.1859 | Train Accuracy: 90.2537 | Accuracy: 90.2300 | F1_score: 0.6250\n",
      "Iteration: 07000 | Train Loss: 0.0558 | Train Accuracy: 90.2537 | Accuracy: 90.2200 | F1_score: 0.6234\n",
      "Iteration: 07500 | Train Loss: 0.1524 | Train Accuracy: 90.2388 | Accuracy: 90.2100 | F1_score: 0.6234\n",
      "Iteration: 08000 | Train Loss: 0.0802 | Train Accuracy: 90.2239 | Accuracy: 90.1700 | F1_score: 0.6212\n",
      "Iteration: 08500 | Train Loss: 0.1448 | Train Accuracy: 90.2090 | Accuracy: 90.1600 | F1_score: 0.6212\n",
      "Iteration: 09000 | Train Loss: 0.1312 | Train Accuracy: 90.2090 | Accuracy: 90.1600 | F1_score: 0.6212\n",
      "Iteration: 09500 | Train Loss: 0.0540 | Train Accuracy: 90.2090 | Accuracy: 90.1600 | F1_score: 0.6212\n",
      "Iteration: 10000 | Train Loss: 0.0769 | Train Accuracy: 90.2090 | Accuracy: 90.1600 | F1_score: 0.6212\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "for epoch in range(int(epochs)):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        labels = labels.long()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels, [1,9], device)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iter+=1\n",
    "        if iter%500==0:\n",
    "            # calculate Accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in train_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total+= labels.size(0)\n",
    "                # for gpu, bring the predicted and labels back to cpu fro python operations to work\n",
    "                correct+= (predicted == labels).sum()\n",
    "            tr_accuracy = 100 * correct/total\n",
    "            predictions = []\n",
    "            test_labels = []\n",
    "            for images, labels in test_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total+= labels.size(0)\n",
    "                # for gpu, bring the predicted and labels back to cpu fro python operations to work\n",
    "                correct+= (predicted == labels).sum()\n",
    "                predictions.extend(predicted)\n",
    "                test_labels.extend(labels)\n",
    "            accuracy = 100 * correct/total\n",
    "            f1_score_val = f1_score(test_labels, predictions)\n",
    "            print(\"Iteration: {:05d} | Train Loss: {:.4f} | Train Accuracy: {:.4f} | Accuracy: {:.4f} | F1_score: {:.4f}\".\n",
    "                  format(iter,loss.item(),tr_accuracy, accuracy, f1_score_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "import random\n",
    "random.seed(0)\n",
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "-Bi-ffLhH8Ir"
   },
   "outputs": [],
   "source": [
    "model1 = LogisticRegression(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "qy1pkNLmH8Is"
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor((1,9), dtype=torch.float32).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "83G4iJWTKc8S"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model1.parameters(), lr=lr_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vHwGsl_QH8It",
    "outputId": "5749f14d-26bf-4460-feaa-40092dd84fef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 00500 | Train Loss: 0.6496 | Train Accuracy: 75.2388 | Accuracy: 75.0300 | F1_score: 0.3783\n",
      "Iteration: 01000 | Train Loss: 0.5271 | Train Accuracy: 80.8060 | Accuracy: 80.4400 | F1_score: 0.4463\n",
      "Iteration: 01500 | Train Loss: 0.4101 | Train Accuracy: 85.1642 | Accuracy: 85.0100 | F1_score: 0.5311\n",
      "Iteration: 02000 | Train Loss: 0.5463 | Train Accuracy: 87.6418 | Accuracy: 87.4700 | F1_score: 0.5771\n",
      "Iteration: 02500 | Train Loss: 0.5361 | Train Accuracy: 88.6567 | Accuracy: 88.4700 | F1_score: 0.5936\n",
      "Iteration: 03000 | Train Loss: 0.3250 | Train Accuracy: 88.5373 | Accuracy: 88.4500 | F1_score: 0.5956\n",
      "Iteration: 03500 | Train Loss: 0.4099 | Train Accuracy: 88.2687 | Accuracy: 88.2800 | F1_score: 0.5954\n",
      "Iteration: 04000 | Train Loss: 0.5511 | Train Accuracy: 87.9701 | Accuracy: 87.9800 | F1_score: 0.5884\n",
      "Iteration: 04500 | Train Loss: 0.4340 | Train Accuracy: 87.9701 | Accuracy: 87.9500 | F1_score: 0.5848\n",
      "Iteration: 05000 | Train Loss: 0.2449 | Train Accuracy: 87.8955 | Accuracy: 87.8800 | F1_score: 0.5819\n",
      "Iteration: 05500 | Train Loss: 0.3767 | Train Accuracy: 87.9254 | Accuracy: 87.8900 | F1_score: 0.5804\n",
      "Iteration: 06000 | Train Loss: 0.3129 | Train Accuracy: 87.8806 | Accuracy: 87.8600 | F1_score: 0.5786\n",
      "Iteration: 06500 | Train Loss: 0.5183 | Train Accuracy: 87.8657 | Accuracy: 87.8200 | F1_score: 0.5741\n",
      "Iteration: 07000 | Train Loss: 0.2395 | Train Accuracy: 87.8955 | Accuracy: 87.8400 | F1_score: 0.5741\n",
      "Iteration: 07500 | Train Loss: 0.4132 | Train Accuracy: 87.9403 | Accuracy: 87.8700 | F1_score: 0.5741\n",
      "Iteration: 08000 | Train Loss: 0.2683 | Train Accuracy: 87.9701 | Accuracy: 87.9000 | F1_score: 0.5747\n",
      "Iteration: 08500 | Train Loss: 0.3253 | Train Accuracy: 88.0000 | Accuracy: 87.9200 | F1_score: 0.5747\n",
      "Iteration: 09000 | Train Loss: 0.2902 | Train Accuracy: 88.0597 | Accuracy: 87.9700 | F1_score: 0.5753\n",
      "Iteration: 09500 | Train Loss: 0.1405 | Train Accuracy: 88.1045 | Accuracy: 88.0100 | F1_score: 0.5759\n",
      "Iteration: 10000 | Train Loss: 0.2905 | Train Accuracy: 88.1343 | Accuracy: 88.0500 | F1_score: 0.5772\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "for epoch in range(int(epochs)):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        labels = labels.long()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model1(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iter+=1\n",
    "        if iter%500==0:\n",
    "            # calculate Accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in train_loader:\n",
    "                outputs = model1(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total+= labels.size(0)\n",
    "                # for gpu, bring the predicted and labels back to cpu fro python operations to work\n",
    "                correct+= (predicted == labels).sum()\n",
    "            tr_accuracy = 100 * correct/total\n",
    "            predictions = []\n",
    "            test_labels = []\n",
    "            for images, labels in test_loader:\n",
    "                outputs = model1(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total+= labels.size(0)\n",
    "                # for gpu, bring the predicted and labels back to cpu fro python operations to work\n",
    "                correct+= (predicted == labels).sum()\n",
    "                predictions.extend(predicted)\n",
    "                test_labels.extend(labels)\n",
    "            accuracy = 100 * correct/total\n",
    "            f1_score_val = f1_score(test_labels, predictions)\n",
    "            print(\"Iteration: {:05d} | Train Loss: {:.4f} | Train Accuracy: {:.4f} | Accuracy: {:.4f} | F1_score: {:.4f}\".\n",
    "                  format(iter,loss.item(),tr_accuracy, accuracy, f1_score_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "import random\n",
    "random.seed(0)\n",
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "othpjJCLKg1H"
   },
   "outputs": [],
   "source": [
    "model2 = LogisticRegression(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "PO1nx08fKg1I"
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor((1,9), dtype=torch.float32).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "bChBr5UnKg1I"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model2.parameters(), lr=lr_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hyqyuHHvKg1J",
    "outputId": "139af290-5e58-4722-e6f0-77e9f329df46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 00500 | Train Loss: 0.4900 | Train Accuracy: 88.1493 | Accuracy: 88.0300 | F1_score: 0.5744\n",
      "Iteration: 01000 | Train Loss: 0.2515 | Train Accuracy: 89.8955 | Accuracy: 89.8400 | F1_score: 0.6254\n",
      "Iteration: 01500 | Train Loss: 0.1621 | Train Accuracy: 89.4328 | Accuracy: 89.4000 | F1_score: 0.6123\n",
      "Iteration: 02000 | Train Loss: 0.5454 | Train Accuracy: 89.2388 | Accuracy: 89.2000 | F1_score: 0.6059\n",
      "Iteration: 02500 | Train Loss: 0.5545 | Train Accuracy: 89.6119 | Accuracy: 89.5000 | F1_score: 0.6093\n",
      "Iteration: 03000 | Train Loss: 0.1411 | Train Accuracy: 89.7164 | Accuracy: 89.5700 | F1_score: 0.6093\n",
      "Iteration: 03500 | Train Loss: 0.2545 | Train Accuracy: 89.8806 | Accuracy: 89.7900 | F1_score: 0.6176\n",
      "Iteration: 04000 | Train Loss: 0.6825 | Train Accuracy: 89.8806 | Accuracy: 89.8400 | F1_score: 0.6228\n",
      "Iteration: 04500 | Train Loss: 0.2525 | Train Accuracy: 90.0597 | Accuracy: 90.0300 | F1_score: 0.6285\n",
      "Iteration: 05000 | Train Loss: 0.0870 | Train Accuracy: 90.0448 | Accuracy: 90.0300 | F1_score: 0.6300\n",
      "Iteration: 05500 | Train Loss: 0.2979 | Train Accuracy: 90.0448 | Accuracy: 90.0300 | F1_score: 0.6300\n",
      "Iteration: 06000 | Train Loss: 0.2146 | Train Accuracy: 90.0597 | Accuracy: 90.0400 | F1_score: 0.6300\n",
      "Iteration: 06500 | Train Loss: 0.5220 | Train Accuracy: 90.0299 | Accuracy: 90.0300 | F1_score: 0.6316\n",
      "Iteration: 07000 | Train Loss: 0.0958 | Train Accuracy: 90.0597 | Accuracy: 90.0500 | F1_score: 0.6316\n",
      "Iteration: 07500 | Train Loss: 0.3300 | Train Accuracy: 90.0597 | Accuracy: 90.0300 | F1_score: 0.6302\n",
      "Iteration: 08000 | Train Loss: 0.2058 | Train Accuracy: 90.0597 | Accuracy: 90.0200 | F1_score: 0.6295\n",
      "Iteration: 08500 | Train Loss: 0.2560 | Train Accuracy: 90.0746 | Accuracy: 90.0300 | F1_score: 0.6295\n",
      "Iteration: 09000 | Train Loss: 0.1939 | Train Accuracy: 90.0597 | Accuracy: 90.0300 | F1_score: 0.6302\n",
      "Iteration: 09500 | Train Loss: 0.0505 | Train Accuracy: 90.0149 | Accuracy: 89.9900 | F1_score: 0.6295\n",
      "Iteration: 10000 | Train Loss: 0.1999 | Train Accuracy: 90.0149 | Accuracy: 90.0000 | F1_score: 0.6302\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "for epoch in range(int(epochs)):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        labels = labels.long()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model2(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iter+=1\n",
    "        if iter%500==0:\n",
    "            # calculate Accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in train_loader:\n",
    "                outputs = model2(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total+= labels.size(0)\n",
    "                # for gpu, bring the predicted and labels back to cpu fro python operations to work\n",
    "                correct+= (predicted == labels).sum()\n",
    "            tr_accuracy = 100 * correct/total\n",
    "            predictions = []\n",
    "            test_labels = []\n",
    "            for images, labels in test_loader:\n",
    "                outputs = model2(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total+= labels.size(0)\n",
    "                # for gpu, bring the predicted and labels back to cpu fro python operations to work\n",
    "                correct+= (predicted == labels).sum()\n",
    "                predictions.extend(predicted)\n",
    "                test_labels.extend(labels)\n",
    "            accuracy = 100 * correct/total\n",
    "            f1_score_val = f1_score(test_labels, predictions)\n",
    "            print(\"Iteration: {:05d} | Train Loss: {:.4f} | Train Accuracy: {:.4f} | Accuracy: {:.4f} | F1_score: {:.4f}\".\n",
    "                  format(iter,loss.item(),tr_accuracy, accuracy, f1_score_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EbUbjgwwK_TB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of synthetic_data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
